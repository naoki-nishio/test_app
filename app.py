import streamlit as st
import streamlit_authenticator as stauth
import pandas as pd
import numpy as np
import pickle
import os
from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer
import warnings
warnings.filterwarnings('ignore')

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="å…¥æœ­å‚åŠ äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ—ï¸"
)

# èªè¨¼è¨­å®š
names = ['é¡§å®¢A', 'é¡§å®¢B', 'admin']
usernames = ['customer_a', 'customer_b', 'admin']

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒãƒƒã‚·ãƒ¥åŒ–ï¼ˆå®Ÿéš›ã®é‹ç”¨ã§ã¯äº‹å‰ã«ç”Ÿæˆï¼‰
passwords = ['password123', 'customer456', 'admin789']
hashed_passwords = stauth.Hasher(passwords).generate()

credentials = {
    'usernames': {
        usernames[0]: {
            'name': names[0],
            'password': hashed_passwords[0]
        },
        usernames[1]: {
            'name': names[1], 
            'password': hashed_passwords[1]
        },
        usernames[2]: {
            'name': names[2],
            'password': hashed_passwords[2]
        }
    }
}

authenticator = stauth.Authenticate(
    credentials,
    'some_cookie_name',
    'some_signature_key',
    cookie_expiry_days=30
)

# ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†
name, authentication_status, username = authenticator.login('ãƒ­ã‚°ã‚¤ãƒ³', 'main')

if authentication_status == False:
    st.error('ãƒ¦ãƒ¼ã‚¶ãƒ¼åã¾ãŸã¯ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé–“é•ã£ã¦ã„ã¾ã™')
elif authentication_status == None:
    st.warning('ãƒ¦ãƒ¼ã‚¶ãƒ¼åã¨ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„')
elif authentication_status:
    # ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸæ™‚ã®å‡¦ç†
    
    # ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³
    authenticator.logout('ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ', 'sidebar')
    st.sidebar.write(f'ã‚ˆã†ã“ã *{name}* ã•ã‚“')
    
    # ã‚¿ã‚¤ãƒˆãƒ«
    st.title("ğŸ—ï¸ å…¥æœ­å‚åŠ äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯ç›¸å¯¾ãƒ‘ã‚¹ã«å¤‰æ›´ï¼‰
    MODEL_FOLDER = "models_2"  # ç›¸å¯¾ãƒ‘ã‚¹ã«å¤‰æ›´
    DATA_PATH = "å…¥æœ­ãƒ‡ãƒ¼ã‚¿_å‡¦ç†æ¸ˆã¿.csv"  # ç›¸å¯¾ãƒ‘ã‚¹ã«å¤‰æ›´
    
    # å¯¾è±¡ä¼æ¥­ãƒªã‚¹ãƒˆ
    target_companies = [
        'é‡‘æœ¬å»ºè¨­ï¼ˆæ ªï¼‰', 'åŒ—æ²¢å»ºè¨­ï¼ˆæ ªï¼‰', 'ï¼ˆæ ªï¼‰ã‚«ãƒªã‚¹', 'å—ä¿¡åœŸæœ¨å»ºç¯‰ï¼ˆæœ‰ï¼‰', 
        'ç¥ç¨²å»ºè¨­ï¼ˆæ ªï¼‰', 'ï¼ˆæ ªï¼‰ãƒ‘ãƒ†ãƒƒã‚¯', 'ï¼ˆæ ªï¼‰ä¸‰å…­çµ„', 'ï¼ˆæ ªï¼‰ãƒˆãƒ©ã‚¤ãƒãƒƒãƒˆ',
        'ï¼ˆæœ‰ï¼‰åŒ—åŸåœŸæœ¨', 'æœ¨ä¸‹å·¥æ¥­ï¼ˆæ ªï¼‰', 'ã‚¯ãƒ©ã‚¦ãƒ‹ãƒ³ã‚°ï¼ˆæ ªï¼‰', 'æœ¨ä¸‹å»ºè¨­ï¼ˆæ ªï¼‰',
        'ç´°æ¾¤å»ºè¨­ï¼ˆæ ªï¼‰', 'å°æœ¨æ›½å»ºè¨­ï¼ˆæ ªï¼‰', 'ï¼ˆæ ªï¼‰æ¸…ä¿¡å»ºè¨­èˆˆæ¥­', 'ï¼ˆæ ªï¼‰ã‚·ãƒãƒ€',
        'å°æ± å»ºè¨­ï¼ˆæ ªï¼‰', 'å®®å˜‰å»ºè¨­ï¼ˆæœ‰ï¼‰', 'å‰å·å»ºè¨­ï¼ˆæ ªï¼‰', 'ï¼ˆæœ‰ï¼‰ç¦å£«çµ„',
        'é•·è±Šå»ºè¨­ï¼ˆæ ªï¼‰', 'é“æ „å»ºè¨­ï¼ˆæ ªï¼‰', 'ä¼Šè³€è‰¯å»ºè¨­ï¼ˆæ ªï¼‰', 'ï¼ˆæœ‰ï¼‰ç•‘ä¸­å·¥å‹™æ‰€',
        'ï¼ˆæ ªï¼‰å°¾ç•‘çµ„', 'ï¼ˆæœ‰ï¼‰ä»Šæ‘å·¥å‹™æ‰€', 'é˜¿æ™ºå·¥å‹™åº—ï¼ˆæ ªï¼‰', 'å‹é–“ç”°å»ºè¨­ï¼ˆæ ªï¼‰',
        'å¤ç”°å·¥æ¥­ï¼ˆæ ªï¼‰', 'å¤§å”å»ºè¨­ï¼ˆæ ªï¼‰', 'ï¼ˆæ ªï¼‰ä¸‹å¹³çµ„', 'ä¸‰å»ºå»ºè¨­ï¼ˆæœ‰ï¼‰',
        'é«˜æœ¬å»ºè¨­ï¼ˆæ ªï¼‰', 'ï¼ˆæœ‰ï¼‰æ–°é‡å·¥å‹™åº—', 'ï¼ˆæœ‰ï¼‰ç«¹æ‘å·¥å‹™æ‰€', 'ï¼ˆæ ªï¼‰ã‚µãƒ³ãƒ†ã‚¯ãƒˆ',
        'ï¼ˆæ ªï¼‰å‰é‡çµ„', 'å¤ªç”°åœŸå»ºï¼ˆæœ‰ï¼‰', 'æ± ç«¯å·¥æ¥­ï¼ˆæ ªï¼‰', 'ï¼ˆæ ªï¼‰è¿‘è—¤å·¥å‹™åº—',
        'çŸ¢æœ¨ã‚³ãƒ¼ãƒãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæ ªï¼‰', 'ï¼ˆæœ‰ï¼‰æ–°äº•å·¥å‹™åº—', 'å…±æ­©ï¼ˆæ ªï¼‰', 'è—¤äº•èˆˆæ¥­ï¼ˆæ ªï¼‰',
        'å—éƒ¨å»ºè¨­ï¼ˆæœ‰ï¼‰', 'å®®ä¸‹å»ºè¨­ï¼ˆæœ‰ï¼‰', 'ï¼ˆæ ªï¼‰é‡‘ç”°çµ„', 'å¤§å¹³å»ºè¨­ï¼ˆæ ªï¼‰',
        'ï¼ˆæœ‰ï¼‰é›„é•·çµ„', 'ï¼ˆæ ªï¼‰å—å»ºè¨­', 'é•·é‡æ©Ÿæï¼ˆæ ªï¼‰', 'å±±å´å»ºè¨­ï¼ˆæ ªï¼‰',
        'ï¼ˆæ ªï¼‰ç‰‡æ¡å·¥å‹™æ‰€', 'ï¼ˆæœ‰ï¼‰ãƒ’ãƒ©ã‚µãƒ¯', 'ï¼ˆæœ‰ï¼‰ã‚¤ãƒãƒåœŸæœ¨', 'ï¼ˆæœ‰ï¼‰å°¾ç•‘çµ„',
        'é£¯ä¼Šæ£®æ—çµ„åˆ'
    ]

    def get_probability_level(probability):
        """ç¢ºç‡ã‚’äºˆæ¸¬ã¨ç¢ºç‡ãƒ¬ãƒ™ãƒ«ã«å¤‰æ›ã™ã‚‹"""
        if probability < 0.5:
            return "ä¸å‚åŠ ", "ä½", 0
        elif probability < 0.75:
            return "å‚åŠ ", "ä¸­", 1
        else:
            return "å‚åŠ ", "é«˜", 2

    @st.cache_data
    def load_encoders():
        """å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ"""
        try:
            df = pd.read_csv(DATA_PATH)
            df = df.drop(columns=['No'], errors='ignore')
            df.fillna('', inplace=True)
            
            def clean_numeric_column(series):
                def convert_to_numeric(x):
                    if pd.isna(x) or x == '' or x == 'éå…¬é–‹' or x == 'äº‹å¾Œå…¬é–‹':
                        return 0
                    try:
                        if isinstance(x, str):
                            x = x.replace(',', '').replace('å††', '')
                        return float(x)
                    except (ValueError, TypeError):
                        return 0
                return series.apply(convert_to_numeric)
            
            df['è³‡æ ¼ç‚¹æ•°'] = clean_numeric_column(df['è³‡æ ¼ç‚¹æ•°'])
            df['äºˆå®šä¾¡æ ¼ï¼ˆç¨æŠœï¼‰'] = clean_numeric_column(df['äºˆå®šä¾¡æ ¼ï¼ˆç¨æŠœï¼‰'])
            df['å·¥ç¨®ãƒªã‚¹ãƒˆ'] = df['å·¥ç¨®ï¼ˆæ¥­ç¨®ï¼‰'].apply(lambda x: str(x).split('ãƒ»'))
            df['å·¥äº‹æ¦‚è¦ãƒªã‚¹ãƒˆ'] = df['å·¥äº‹æ¦‚è¦ï¼ˆæ¥­å‹™æ¦‚è¦ï¼‰'].apply(lambda x: str(x).split())
            
            mlb_type = MultiLabelBinarizer()
            mlb_type.fit(df['å·¥ç¨®ãƒªã‚¹ãƒˆ'])
            
            mlb_summary = MultiLabelBinarizer()
            mlb_summary.fit(df['å·¥äº‹æ¦‚è¦ãƒªã‚¹ãƒˆ'])
            
            ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
            ohe.fit(df[['å…¥æœ­æ–¹å¼', 'ç™ºæ³¨æ©Ÿé–¢', 'å·¥äº‹å ´æ‰€']])
            
            return mlb_type, mlb_summary, ohe
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None, None

    def load_company_model(company_name):
        """ä¼æ¥­åˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        model_filename = f"{company_name.replace('/', '_').replace('ï¼ˆ', '(').replace('ï¼‰', ')')}_smote_xgboost.pkl"
        model_path = os.path.join(MODEL_FOLDER, model_filename)
        
        if not os.path.exists(model_path):
            return None
        
        try:
            with open(model_path, 'rb') as f:
                model = pickle.load(f)
            return model
        except:
            return None

    def create_feature_vector(bidding_method, agency, location, qualification_score, 
                             planned_price, work_type, work_summary, mlb_type, mlb_summary, ohe):
        """ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ"""
        try:
            # æ•°å€¤ç‰¹å¾´é‡
            X_num = np.array([qualification_score, planned_price])
            
            # å·¥ç¨®ãƒ»å·¥äº‹æ¦‚è¦
            work_type_list = [str(work_type).split('ãƒ»')]
            summary_list = [str(work_summary).split()]
            
            X_type = mlb_type.transform(work_type_list)
            X_summary = mlb_summary.transform(summary_list)
            
            # ã‚«ãƒ†ã‚´ãƒª
            cat_data = pd.DataFrame({
                'å…¥æœ­æ–¹å¼': [bidding_method],
                'ç™ºæ³¨æ©Ÿé–¢': [agency],
                'å·¥äº‹å ´æ‰€': [location]
            })
            X_cat = ohe.transform(cat_data)
            
            # ç‰¹å¾´é‡çµåˆ
            X = np.hstack([X_num, X_type.flatten(), X_summary.flatten(), X_cat.flatten()])
            return X
        except Exception as e:
            st.error(f"ç‰¹å¾´é‡ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼èª­ã¿è¾¼ã¿
    mlb_type, mlb_summary, ohe = load_encoders()

    if mlb_type is None:
        st.error("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        st.stop()

    # å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    st.header("æ¡ˆä»¶æƒ…å ±å…¥åŠ›")

    col1, col2 = st.columns(2)

    with col1:
        bidding_method = st.text_input("å…¥æœ­æ–¹å¼", value="")
        agency = st.text_input("ç™ºæ³¨æ©Ÿé–¢", value="")
        location = st.text_input("å·¥äº‹å ´æ‰€", value="")

    with col2:
        qualification_score = st.number_input("è³‡æ ¼ç‚¹æ•°", min_value=0, max_value=1500, value=0)
        planned_price = st.number_input("äºˆå®šä¾¡æ ¼ï¼ˆå††ï¼‰", min_value=0, value=0, step=1000000)
        work_type = st.text_input("å·¥ç¨®", value="")

    work_summary = st.text_area("å·¥äº‹æ¦‚è¦", value="", height=100)

    # äºˆæ¸¬å®Ÿè¡Œ
    if st.button("äºˆæ¸¬å®Ÿè¡Œ"):
        
        # ç‰¹å¾´é‡ä½œæˆ
        features = create_feature_vector(
            bidding_method, agency, location, qualification_score,
            planned_price, work_type, work_summary, mlb_type, mlb_summary, ohe
        )
        
        if features is None:
            st.error("ç‰¹å¾´é‡ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            st.stop()
        
        # äºˆæ¸¬å®Ÿè¡Œ
        predictions = []
        
        for company in target_companies:
            model = load_company_model(company)
            
            if model is not None:
                try:
                    features_2d = features.reshape(1, -1)
                    prediction = model.predict(features_2d)[0]
                    probability = model.predict_proba(features_2d)[0][1]
                    
                    prediction_result, prob_level, sort_key = get_probability_level(probability)
                    
                    predictions.append({
                        'ä¼æ¥­å': company,
                        'äºˆæ¸¬': prediction_result,
                        'ç¢ºç‡': prob_level,
                        'ã‚½ãƒ¼ãƒˆã‚­ãƒ¼': sort_key,
                        'ç¢ºç‡æ•°å€¤': probability
                    })
                except:
                    predictions.append({
                        'ä¼æ¥­å': company,
                        'äºˆæ¸¬': 'ã‚¨ãƒ©ãƒ¼',
                        'ç¢ºç‡': '-',
                        'ã‚½ãƒ¼ãƒˆã‚­ãƒ¼': -1,
                        'ç¢ºç‡æ•°å€¤': 0
                    })
            else:
                predictions.append({
                    'ä¼æ¥­å': company,
                    'äºˆæ¸¬': 'ãƒ¢ãƒ‡ãƒ«ãªã—',
                    'ç¢ºç‡': '-',
                    'ã‚½ãƒ¼ãƒˆã‚­ãƒ¼': -1,
                    'ç¢ºç‡æ•°å€¤': 0
                })
        
        # çµæœè¡¨ç¤º
        results_df = pd.DataFrame(predictions)
        results_df = results_df.sort_values('ã‚½ãƒ¼ãƒˆã‚­ãƒ¼', ascending=False)
        
        st.header("äºˆæ¸¬çµæœ")
        
        # ã‚µãƒãƒªãƒ¼
        participating = results_df[results_df['äºˆæ¸¬'] == 'å‚åŠ ']
        high_prob = results_df[results_df['ç¢ºç‡'] == 'é«˜']
        medium_prob = results_df[results_df['ç¢ºç‡'] == 'ä¸­']
        low_prob = results_df[results_df['ç¢ºç‡'] == 'ä½']
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("å‚åŠ äºˆæ¸¬ä¼æ¥­æ•°", len(participating))
        with col2:
            st.metric("ç¢ºç‡é«˜", len(high_prob))
        with col3:
            st.metric("ç¢ºç‡ä¸­", len(medium_prob))
        with col4:
            st.metric("ç¢ºç‡ä½", len(low_prob))
        
        # çµæœãƒ†ãƒ¼ãƒ–ãƒ«
        display_df = results_df[['ä¼æ¥­å', 'äºˆæ¸¬', 'ç¢ºç‡']].copy()
        st.dataframe(display_df, use_container_width=True, height=400)